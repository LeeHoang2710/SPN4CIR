@misc{bai2023sentencelevelpromptsbenefitcomposed,
    archiveprefix = {arXiv},
    author        = {Yang Bai and Xinxing Xu and Yong Liu and Salman Khan and Fahad Khan and Wangmeng Zuo and Rick Siow Mong Goh and Chun-Mei Feng},
    eprint        = {2310.05473},
    primaryclass  = {cs.CV},
    title         = {Sentence-level Prompts Benefit Composed Image Retrieval},
    url           = {https://arxiv.org/abs/2310.05473},
    year          = {2023}
}
@article{Chen_Zhong_He_Peng_Zhou_Cheng_2024,
    abstractnote = {The goal of composed fashion image retrieval is to locate a target image based on a reference image and modified text. Recent methods utilize symmetric encoders (e.g., CLIP) pre-trained on large-scale non-fashion datasets. However, the input for this task exhibits an asymmetric nature, where the reference image contains rich content while the modified text is often brief. Therefore, methods employing symmetric encoders encounter a severe phenomenon: retrieval results dominated by reference images, leading to the oversight of modified text. We propose a Fashion Enhance-and-Refine Network (FashionERN) centered around two aspects: enhancing the text encoder and refining visual semantics. We introduce a Triple-branch Modifier Enhancement model, which injects relevant information from the reference image and aligns the modified text modality with the target image modality. Furthermore, we propose a Dual-guided Vision Refinement model that retains critical visual information through text-guided refinement and self-guided refinement processes. The combination of these two models significantly mitigates the reference dominance phenomenon, ensuring accurate fulfillment of modifier requirements. Comprehensive experiments demonstrate our approachâ€™s state-of-the-art performance on four commonly used datasets.},
    author       = {Chen, Yanzhe and Zhong, Huasong and He, Xiangteng and Peng, Yuxin and Zhou, Jiahuan and Cheng, Lele},
    doi          = {10.1609/aaai.v38i2.27885},
    journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
    month        = {Mar.},
    number       = {2},
    pages        = {1228-1236},
    title        = {FashionERN: Enhance-and-Refine Network for Composed Fashion Image Retrieval},
    url          = {https://ojs.aaai.org/index.php/AAAI/article/view/27885},
    volume       = {38},
    year         = {2024}
}
@misc{feng2024improvingcomposedimageretrieval,
    archiveprefix = {arXiv},
    author        = {Zhangchi Feng and Richong Zhang and Zhijie Nie},
    eprint        = {2404.11317},
    primaryclass  = {cs.CV},
    title         = {Improving Composed Image Retrieval via Contrastive Learning with Scaling Positives and Negatives},
    url           = {https://arxiv.org/abs/2404.11317},
    year          = {2024}
}
@misc{zhao2024unifashionunifiedvisionlanguagemodel,
    archiveprefix = {arXiv},
    author        = {Xiangyu Zhao and Yuehan Zhang and Wenlong Zhang and Xiao-Ming Wu},
    eprint        = {2408.11305},
    primaryclass  = {cs.CV},
    title         = {UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation},
    url           = {https://arxiv.org/abs/2408.11305},
    year          = {2024}
}