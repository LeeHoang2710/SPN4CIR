\FloatBarrier
\section{Related Work}
\textbf{Composed image retrieval (CIR)} combines a query image with textual descriptions to retrieve target images, a challenging task requiring effective fusion of visual and textual modalities. Early approaches, such as Vo et al., 2019~\cite{Vo_2019_CVPR}, explored residual connection-based methods for integrating queries. Recent work leverages vision-language models: Liu et al., 2021~\cite{Liu_2021_ICCV} introduced CIRPLANT with transformer-based adaptation, while Baldrati et al., 2022, 2023~\cite{Baldrati_2022_CVPR,baldrati2023composedimageretrievalusing} utilized CLIP for feature fusion. Xu et al.~\cite{10.1109/TMM.2023.3235495} proposed ComqueryFormer with a unified transformer architecture and global-local alignment. Zhao et al., 2022~\cite{zhao2022progressivelearningimageretrieval} introduced progressive learning with adaptive weighting for hybrid queries, and Bai et al., 2023~\cite{bai2023sentencelevelpromptsbenefitcomposed} enhanced retrieval using sentence-level prompts from pretrained models. Other innovations include zero-shot methods (Saito et al.~\cite{Saito_2023_CVPR}), context-aware mapping (Tang et al.~\cite{Tang_Yu_Gai_Zhuang_Xiong_Hu_Wu_2024}), and application to video retrieval (Ventura et al.~\cite{Ventura_2024}).

Recent advances in \textbf{data generation for CIR} focus on synthetic data and multimodal retrieval techniques. CompoDiff~\cite{gu2024compodiffversatilecomposedimage}, using latent diffusion, achieves state-of-the-art performance on benchmarks like Fashion IQ~\cite{wu2020fashioniqnewdataset}, and CIRR~\cite{Liu_2021_ICCV}, through automated triplet generation. MagicLens~\cite{zhang2024magiclensselfsupervisedimageretrieval} improves retrieval via self-supervised learning with synthesized embeddings, while InstructPix2Pix~\cite{brooks2023instructpix2pixlearningfollowimage} generates text and images for compositional tasks using GPT-3 and diffusion models. Efforts like UniIR~\cite{wei2023uniirtrainingbenchmarkinguniversal} integrate vision-language queries to enhance retrieval efficiency and accuracy across diverse datasets.

\textbf{Negative sampling} plays a vital role in contrastive learning for image retrieval. Feng et al.~\cite{feng2024improvingcomposedimageretrieval} used multi-modal language models to generate triplets for CIR, addressing positive data scarcity. Zhou and Li~\cite{zhou2024coarsetofinealignmentmakesbetter} proposed a coarse-to-fine alignment framework for cross-modal image retrieval, improving performance through targeted sampling. Contrastive learning approaches, such as SimCLR~\cite{chen2020simpleframeworkcontrastivelearning} and non-parametric instance discrimination~\cite{wu2018unsupervisedfeaturelearningnonparametric}, have shown that augmentations and effective sampling of negatives are essential for learning robust representations. Additionally, conditional negative sampling~\cite{wu2020conditionalnegativesamplingcontrastive} enhances feature transferability to new distributions, while contrastive hashing with vision transformer~\cite{https://doi.org/10.1002/int.23082} improves retrieval performance by integrating hard negative samples.